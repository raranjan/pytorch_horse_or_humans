{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Timing utility\n",
    "from timeit import default_timer as timer\n",
    "import time\n",
    "import copy\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "# Setting device on GPU if available, else CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VGG-16 Takes 224x224 images as input, so we resize all the training and validation data\n",
    "train_data_path = \"data/train\"\n",
    "valid_data_path = \"data/validation\"\n",
    "batch_size = 128 # Batch size will depend on the hardware\n",
    "save_file_name = 'vgg16-transfer-4.pt'\n",
    "checkpoint_path = 'vgg16-transfer-4.pth'\n",
    "\n",
    "# Image transformations. Below are the key steps we take:\n",
    "# 1. Resize\n",
    "# 2. Center crop to 224 x 224\n",
    "# 3. Convert to a tensor\n",
    "# 4. Normalize with mean and standard deviation\n",
    "image_transforms = {\n",
    "    # Train uses data augmentation (Artificially increase the number of training image)\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(), # Randomly change the brightness, contrast and saturation of an image\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),  # Image net standards\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Imagenet standards\n",
    "    ]),\n",
    "    # Validation does not use augmentation\n",
    "    'valid':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using DataLoaders to avoid loading all of the data into memory at once. \n",
    "# First, we create a dataset object from the image folders, and then we pass these to a DataLoader. \n",
    "# At training time, the DataLoader will load the images from disk, apply the transformations, and yield a batch. \n",
    "# We'll iterate through all the batches in the respective DataLoader.\n",
    "\n",
    "# Datasets from folders\n",
    "data = {\n",
    "    'train': datasets.ImageFolder(root=train_data_path, transform=image_transforms['train']),\n",
    "    'valid': datasets.ImageFolder(root=valid_data_path, transform=image_transforms['valid']),\n",
    "}\n",
    "\n",
    "# Dataloader iterators with shuffle\n",
    "dataloaders = {\n",
    "    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True),\n",
    "    'valid': DataLoader(data['valid'], batch_size=batch_size, shuffle=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes in the dataset: ['horses', 'humans']\n"
     ]
    }
   ],
   "source": [
    "classes = data['train'].classes\n",
    "dataset_sizes = {x: len(data[x]) for x in ['train', 'valid']}\n",
    "\n",
    "print(f\"Total classes in the dataset: {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transfer Learning using pre-trained vggnet models\n",
    "The idea behind pre-training is the early convolutional layers of a cnn extract features that are relevant for many image recognition tasks. The later, fully-connected layers, specialize to the specific dataset by learning higher-level features. Therefore, we can use the already trained convolutional layers while training only the fully-connected layers on our own dataset.\n",
    "\n",
    "The complete list of models can be found [here](https://pytorch.org/docs/stable/torchvision/models.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## VGG16 - Classifier\n",
    "The classifier is the part of the model that we'll train. However, for the vgg, we'll only need to train the last few layers in the classifier and not even all of the fully connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Add on Custom Classifier\n",
    "We'll train a classifier consisting of the following layers\n",
    "\n",
    "Fully connected with ReLU activation (n_inputs, 256)\n",
    "Dropout with 40% chance of dropping\n",
    "Fully connected with log softmax output (256, n_classes)\n",
    "\n",
    "To build our custom classifier, we use the nn.Sequential() module which allows us to specify each layer one after the other. We assign our custom classifier to the final classifier layer in the already trained vgg network. When we add on the extra layers, they are set to require_grad=True by default. These will be the only layers that are trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(name=\"vgg16\"):\n",
    "    # if model_name ==\"vgg16\":\n",
    "    if True:\n",
    "        model = models.vgg16(pretrained=True)\n",
    "\n",
    "        # We freeze all of the existing layers in the network by setting requires_grad to False.\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        n_inputs = model.classifier[6].in_features\n",
    "        n_classes = len(classes)\n",
    "        print(\"No of inputs: {}\".format(n_inputs))\n",
    "        print(\"No of classes: {}\".format(n_classes))\n",
    "\n",
    "        # Add on classifier\n",
    "        model.classifier[6] = nn.Sequential(\n",
    "            nn.Linear(n_inputs, 256), \n",
    "            nn.ReLU(), nn.Dropout(0.4),\n",
    "            nn.Linear(256, n_classes), \n",
    "            nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of inputs: 4096\n",
      "No of classes: 2\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [128, 64, 224, 224]           1,792\n",
      "              ReLU-2        [128, 64, 224, 224]               0\n",
      "            Conv2d-3        [128, 64, 224, 224]          36,928\n",
      "              ReLU-4        [128, 64, 224, 224]               0\n",
      "         MaxPool2d-5        [128, 64, 112, 112]               0\n",
      "            Conv2d-6       [128, 128, 112, 112]          73,856\n",
      "              ReLU-7       [128, 128, 112, 112]               0\n",
      "            Conv2d-8       [128, 128, 112, 112]         147,584\n",
      "              ReLU-9       [128, 128, 112, 112]               0\n",
      "        MaxPool2d-10         [128, 128, 56, 56]               0\n",
      "           Conv2d-11         [128, 256, 56, 56]         295,168\n",
      "             ReLU-12         [128, 256, 56, 56]               0\n",
      "           Conv2d-13         [128, 256, 56, 56]         590,080\n",
      "             ReLU-14         [128, 256, 56, 56]               0\n",
      "           Conv2d-15         [128, 256, 56, 56]         590,080\n",
      "             ReLU-16         [128, 256, 56, 56]               0\n",
      "        MaxPool2d-17         [128, 256, 28, 28]               0\n",
      "           Conv2d-18         [128, 512, 28, 28]       1,180,160\n",
      "             ReLU-19         [128, 512, 28, 28]               0\n",
      "           Conv2d-20         [128, 512, 28, 28]       2,359,808\n",
      "             ReLU-21         [128, 512, 28, 28]               0\n",
      "           Conv2d-22         [128, 512, 28, 28]       2,359,808\n",
      "             ReLU-23         [128, 512, 28, 28]               0\n",
      "        MaxPool2d-24         [128, 512, 14, 14]               0\n",
      "           Conv2d-25         [128, 512, 14, 14]       2,359,808\n",
      "             ReLU-26         [128, 512, 14, 14]               0\n",
      "           Conv2d-27         [128, 512, 14, 14]       2,359,808\n",
      "             ReLU-28         [128, 512, 14, 14]               0\n",
      "           Conv2d-29         [128, 512, 14, 14]       2,359,808\n",
      "             ReLU-30         [128, 512, 14, 14]               0\n",
      "        MaxPool2d-31           [128, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32           [128, 512, 7, 7]               0\n",
      "           Linear-33                [128, 4096]     102,764,544\n",
      "             ReLU-34                [128, 4096]               0\n",
      "          Dropout-35                [128, 4096]               0\n",
      "           Linear-36                [128, 4096]      16,781,312\n",
      "             ReLU-37                [128, 4096]               0\n",
      "          Dropout-38                [128, 4096]               0\n",
      "           Linear-39                 [128, 256]       1,048,832\n",
      "             ReLU-40                 [128, 256]               0\n",
      "          Dropout-41                 [128, 256]               0\n",
      "           Linear-42                   [128, 2]             514\n",
      "       LogSoftmax-43                   [128, 2]               0\n",
      "================================================================\n",
      "Total params: 135,309,890\n",
      "Trainable params: 1,049,346\n",
      "Non-trainable params: 134,260,544\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 73.50\n",
      "Forward/backward pass size (MB): 28003.75\n",
      "Params size (MB): 516.17\n",
      "Estimated Total Size (MB): 28593.42\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "summary(model, input_size=(3, 224, 224), batch_size=batch_size, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 4096])\n",
      "torch.Size([256])\n",
      "torch.Size([2, 256])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# The parameters (weights) that will be updated by the optimizer during training\n",
    "for p in optimizer.param_groups[0]['params']:\n",
    "    if p.requires_grad:\n",
    "        print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, scheduler, n_epochs=25):\n",
    "    since = time.time()\n",
    "    \n",
    "    # A shallow copy constructs a new compound object and then inserts references into it to the objects found in the original. \n",
    "    # A deep copy constructs a new compound object and then, recursively, inserts copies into it of the objects found in the original.\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(f'Epoch {epoch}/{n_epochs-1}')\n",
    "        print('-'*20)\n",
    "        \n",
    "        # Each epoch has training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train() # Set model to training mode\n",
    "            else:\n",
    "                model.eval() # Set model to validation mode\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            \n",
    "            # Iterate over data\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Set gradients to 0\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward\n",
    "                # Track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1) # Returns values, indices where values is the maximum value of each row of the input tensor\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # Backward + Optimize only if in training\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            #Deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "        print()\n",
    "    time_elasped = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elasped // 60, time_elasped % 60))\n",
    "    print('Best val Acc: {:.4f}'.format(best_acc))\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "--------------------\n",
      "train Loss: 0.1498 Acc: 0.9367\n",
      "valid Loss: 0.0078 Acc: 1.0000\n",
      "\n",
      "Epoch 1/24\n",
      "--------------------\n",
      "train Loss: 0.0055 Acc: 0.9990\n",
      "valid Loss: 0.0061 Acc: 0.9961\n",
      "\n",
      "Epoch 2/24\n",
      "--------------------\n",
      "train Loss: 0.0030 Acc: 0.9981\n",
      "valid Loss: 0.0023 Acc: 1.0000\n",
      "\n",
      "Epoch 3/24\n",
      "--------------------\n",
      "train Loss: 0.0067 Acc: 0.9971\n",
      "valid Loss: 0.0055 Acc: 0.9961\n",
      "\n",
      "Epoch 4/24\n",
      "--------------------\n",
      "train Loss: 0.0082 Acc: 0.9990\n",
      "valid Loss: 0.0089 Acc: 0.9922\n",
      "\n",
      "Epoch 5/24\n",
      "--------------------\n",
      "train Loss: 0.0070 Acc: 0.9981\n",
      "valid Loss: 0.0013 Acc: 1.0000\n",
      "\n",
      "Epoch 6/24\n",
      "--------------------\n",
      "train Loss: 0.0064 Acc: 0.9981\n",
      "valid Loss: 0.0033 Acc: 1.0000\n",
      "\n",
      "Epoch 7/24\n",
      "--------------------\n",
      "train Loss: 0.0073 Acc: 0.9981\n",
      "valid Loss: 0.0033 Acc: 1.0000\n",
      "\n",
      "Epoch 8/24\n",
      "--------------------\n",
      "train Loss: 0.0006 Acc: 1.0000\n",
      "valid Loss: 0.0030 Acc: 1.0000\n",
      "\n",
      "Epoch 9/24\n",
      "--------------------\n",
      "train Loss: 0.0011 Acc: 1.0000\n",
      "valid Loss: 0.0031 Acc: 1.0000\n",
      "\n",
      "Epoch 10/24\n",
      "--------------------\n",
      "train Loss: 0.0032 Acc: 0.9981\n",
      "valid Loss: 0.0028 Acc: 1.0000\n",
      "\n",
      "Epoch 11/24\n",
      "--------------------\n",
      "train Loss: 0.0021 Acc: 0.9990\n",
      "valid Loss: 0.0028 Acc: 1.0000\n",
      "\n",
      "Epoch 12/24\n",
      "--------------------\n",
      "train Loss: 0.0045 Acc: 0.9971\n",
      "valid Loss: 0.0033 Acc: 1.0000\n",
      "\n",
      "Epoch 13/24\n",
      "--------------------\n",
      "train Loss: 0.0010 Acc: 1.0000\n",
      "valid Loss: 0.0036 Acc: 1.0000\n",
      "\n",
      "Epoch 14/24\n",
      "--------------------\n",
      "train Loss: 0.0018 Acc: 1.0000\n",
      "valid Loss: 0.0036 Acc: 1.0000\n",
      "\n",
      "Epoch 15/24\n",
      "--------------------\n",
      "train Loss: 0.0001 Acc: 1.0000\n",
      "valid Loss: 0.0036 Acc: 1.0000\n",
      "\n",
      "Epoch 16/24\n",
      "--------------------\n",
      "train Loss: 0.0003 Acc: 1.0000\n",
      "valid Loss: 0.0036 Acc: 1.0000\n",
      "\n",
      "Epoch 17/24\n",
      "--------------------\n",
      "train Loss: 0.0027 Acc: 0.9990\n",
      "valid Loss: 0.0036 Acc: 1.0000\n",
      "\n",
      "Epoch 18/24\n",
      "--------------------\n",
      "train Loss: 0.0022 Acc: 0.9990\n",
      "valid Loss: 0.0036 Acc: 1.0000\n",
      "\n",
      "Epoch 19/24\n",
      "--------------------\n",
      "train Loss: 0.0005 Acc: 1.0000\n",
      "valid Loss: 0.0035 Acc: 1.0000\n",
      "\n",
      "Epoch 20/24\n",
      "--------------------\n",
      "train Loss: 0.0005 Acc: 1.0000\n",
      "valid Loss: 0.0035 Acc: 1.0000\n",
      "\n",
      "Epoch 21/24\n",
      "--------------------\n",
      "train Loss: 0.0039 Acc: 0.9990\n",
      "valid Loss: 0.0035 Acc: 1.0000\n",
      "\n",
      "Epoch 22/24\n",
      "--------------------\n",
      "train Loss: 0.0004 Acc: 1.0000\n",
      "valid Loss: 0.0035 Acc: 1.0000\n",
      "\n",
      "Epoch 23/24\n",
      "--------------------\n",
      "train Loss: 0.0015 Acc: 0.9990\n",
      "valid Loss: 0.0035 Acc: 1.0000\n",
      "\n",
      "Epoch 24/24\n",
      "--------------------\n",
      "train Loss: 0.0018 Acc: 0.9990\n",
      "valid Loss: 0.0035 Acc: 1.0000\n",
      "\n",
      "Training complete in 425m 53s\n",
      "Best val Acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "model = train(model, criterion, optimizer, exp_lr_scheduler, n_epochs=25)\n",
    "torch.save(model.state_dict(), 'horse_or_human.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "nteract": {
   "version": "nteract-on-jupyter@2.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
